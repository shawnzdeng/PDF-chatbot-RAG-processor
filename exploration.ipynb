{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fe9cec",
   "metadata": {},
   "source": [
    "   \"# PDF RAG Parameter Optimization Exploration\n",
    "\",\n",
    "    \"\n",
    "\",\n",
    "    \"This notebook demonstrates the parameter optimization system for RAG with Qdrant cloud integration. The focus is on finding optimal parameters for production RAG systems.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85becb31",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c727e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our modules\n",
    "from config import Config\n",
    "from processor import QdrantProcessor\n",
    "from simple_rag import QdrantRAG\n",
    "from parameter_tuning import ParameterTuner\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path: {sys.path[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167beb33",
   "metadata": {},
   "source": [
    "## Environment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff483d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate environment variables\n",
    "try:\n",
    "    Config.validate_env_vars()\n",
    "    print(\"‚úÖ Environment variables validated\")\n",
    "    print(f\"OpenAI API Key: {Config.OPENAI_API_KEY[:20]}...\")\n",
    "    print(f\"Qdrant URL: {Config.QDRANT_URL}\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Environment validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b065ddc",
   "metadata": {},
   "source": [
    "## PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = QdrantProcessor(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    "    collection_name=\"pdf_documents_notebook\"\n",
    ")\n",
    "\n",
    "print(f\"Processor initialized with:\")\n",
    "print(f\"  Chunk size: {processor.chunk_size}\")\n",
    "print(f\"  Chunk overlap: {processor.chunk_overlap}\")\n",
    "print(f\"  Embedding model: {processor.embedding_model}\")\n",
    "print(f\"  Collection: {processor.collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42058973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process PDF\n",
    "pdf_path = Config.DATA_DIR / \"random_machine_learing_pdf.pdf\"\n",
    "\n",
    "if pdf_path.exists():\n",
    "    print(f\"Processing PDF: {pdf_path}\")\n",
    "    success = processor.process_pdf_to_qdrant(str(pdf_path))\n",
    "    \n",
    "    if success:\n",
    "        print(\"‚úÖ PDF processed successfully\")\n",
    "        collection_info = processor.get_collection_info()\n",
    "        print(f\"Collection info: {collection_info}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to process PDF\")\n",
    "else:\n",
    "    print(f\"‚ùå PDF file not found: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af34836",
   "metadata": {},
   "source": [
    "## RAG System Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG system\n",
    "rag = QdrantRAG(\n",
    "    collection_name=\"pdf_documents_notebook\",\n",
    "    llm_model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"RAG system initialized\")\n",
    "print(f\"Collection stats: {rag.get_collection_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What are the types of learning?\",\n",
    "    \"What is PAC learning?\",\n",
    "    \"Describe the goal of reinforcement learning.\",\n",
    "    \"What is entropy in decision trees?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = rag.answer_question(question)\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Retrieved documents: {result['retrieved_documents']}\")\n",
    "    print(f\"  Average relevance: {result['average_relevance_score']:.3f}\")\n",
    "    print(f\"  Sources: {result['sources'][:2]}...\")  # Show first 2 sources\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9258c",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create results dataframe\n",
    "results_data = []\n",
    "for result in results:\n",
    "    results_data.append({\n",
    "        'question': result['question'][:50] + '...',  # Truncate for display\n",
    "        'answer_length': len(result['answer']),\n",
    "        'retrieved_docs': result['retrieved_documents'],\n",
    "        'avg_relevance': result['average_relevance_score']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(\"Results Summary:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40915d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relevance scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(df_results)), df_results['avg_relevance'])\n",
    "plt.title('Average Relevance Scores')\n",
    "plt.xlabel('Question Index')\n",
    "plt.ylabel('Relevance Score')\n",
    "plt.xticks(range(len(df_results)), [f'Q{i+1}' for i in range(len(df_results))])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(df_results)), df_results['retrieved_docs'])\n",
    "plt.title('Retrieved Documents Count')\n",
    "plt.xlabel('Question Index')\n",
    "plt.ylabel('Document Count')\n",
    "plt.xticks(range(len(df_results)), [f'Q{i+1}' for i in range(len(df_results))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9db137",
   "metadata": {},
   "source": [
    "## Parameter Tuning (Small Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909134f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small scale parameter tuning for demonstration\n",
    "tuner = ParameterTuner()\n",
    "\n",
    "# Override config for smaller test\n",
    "tuner.config = {\n",
    "    \"chunk_sizes\": [500, 1000],\n",
    "    \"chunk_overlaps\": [100, 200], \n",
    "    \"embedding_models\": [\"text-embedding-3-small\"],\n",
    "    \"llm_models\": [\"gpt-3.5-turbo\"],\n",
    "    \"temperatures\": [0.0, 0.3],\n",
    "    \"top_k_retrieval\": [3, 5],\n",
    "    \"experiment_settings\": {\n",
    "        \"max_combinations\": 4,\n",
    "        \"random_seed\": 42\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Starting small-scale parameter tuning...\")\n",
    "print(f\"Will test {tuner.config['experiment_settings']['max_combinations']} combinations\")\n",
    "\n",
    "# Generate combinations\n",
    "combinations = tuner.generate_parameter_combinations(max_combinations=4)\n",
    "print(f\"\\nParameter combinations to test:\")\n",
    "for i, combo in enumerate(combinations):\n",
    "    print(f\"  {i+1}: {combo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a70f2",
   "metadata": {},
   "source": [
    "## Custom Query Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query testing\n",
    "def test_custom_query(question, top_k=5):\n",
    "    \"\"\"Test a custom query with detailed output\"\"\"\n",
    "    print(f\"\\nüîç Testing Query: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get detailed retrieval results\n",
    "    retrieved_docs = rag.retrieve_documents(question, top_k=top_k)\n",
    "    \n",
    "    print(f\"\\nüìÑ Retrieved {len(retrieved_docs)} documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"\\nDocument {i+1} (Score: {doc.score:.3f}):\")\n",
    "        print(f\"  Content: {doc.content[:200]}...\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "    \n",
    "    # Get full answer\n",
    "    result = rag.answer_question(question, top_k=top_k)\n",
    "    \n",
    "    print(f\"\\nüí° Final Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test a custom question\n",
    "custom_result = test_custom_query(\"What is the difference between supervised and unsupervised learning?\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084e0be",
   "metadata": {},
   "source": [
    "## System Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0aae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance testing\n",
    "performance_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Define entropy.\",\n",
    "    \"What is reinforcement learning?\"\n",
    "]\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for question in performance_questions:\n",
    "    start_time = time.time()\n",
    "    result = rag.answer_question(question)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    performance_results.append({\n",
    "        'question': question,\n",
    "        'response_time': end_time - start_time,\n",
    "        'answer_length': len(result['answer']),\n",
    "        'retrieved_docs': result['retrieved_documents'],\n",
    "        'relevance_score': result['average_relevance_score']\n",
    "    })\n",
    "\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "print(\"Performance Analysis:\")\n",
    "print(perf_df)\n",
    "print(f\"\\nAverage response time: {perf_df['response_time'].mean():.2f} seconds\")\n",
    "print(f\"Average relevance score: {perf_df['relevance_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58af55",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff54e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä RAG System Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ PDF processed and stored in Qdrant\")\n",
    "print(f\"‚úÖ RAG system operational\")\n",
    "print(f\"‚úÖ {len(test_questions)} test questions processed\")\n",
    "print(f\"‚úÖ Average relevance score: {df_results['avg_relevance'].mean():.3f}\")\n",
    "print(f\"‚úÖ Average response time: {perf_df['response_time'].mean():.2f}s\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. Run full parameter tuning: python main.py tune\")\n",
    "print(\"2. Launch Streamlit app: python main.py app\")\n",
    "print(\"3. Try interactive CLI: python main.py query --interactive\")\n",
    "print(\"4. Add more PDFs to data/ folder and reprocess\")\n",
    "print(\"5. Customize prompts and evaluation metrics\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
